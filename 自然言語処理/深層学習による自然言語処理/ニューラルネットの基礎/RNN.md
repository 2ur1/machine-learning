# 再帰ニューラルネット(recursive neural networks: RNN)
## モデル
> RNN: 可変長の入力列を扱うことに優れたネットワーク構造
> RNNは前の時刻の隠れ状態ベクトルと現時刻の入力ベクトル(または下層の隠れ状態ベクトル)を使って、現在の隠れ状態ベクトルを更新する。こうすることで、任意の長さの入力履歴を考慮した出力を得ることができるようになる。
>
> - 状態変数の数を十分大きくとれば原理的には前の入力すべてをRNNによって記憶したうえで予測をすることができる
> - パラメータを時刻非依存とすることで、任意の長さの入力列を同じRNNで扱うことができる
>
> 単語などを各時刻の入力と思うとテキストは可変長の入力列となるので、同じモデルで任意の長さの入力列を扱えるRNNはテキスト処理に向いているモデルといえる。

RNNは長さ$T$の入力ベクトル列
$$
X = \left( x_{1}, x_{2}, \cdots, x_{T} \right)
$$

 - $X$: 行列
 - $x_{i} \quad (i = 1,2,\cdots,T)$: 入力ベクトル

が与えられたときに、$\iota$層目の隠れ状態ベクトルを次のように再帰的に更新する。
$$
h_{ t }^{ \iota  }=a^{ (\iota ) }\left( W^{ (\iota ) }\left[ \begin{matrix} { h }_{ t }^{ (\iota -1) } \\ { h }_{ t-1 }^{ (\iota ) } \end{matrix} \right] +{ b }^{ (\iota ) } \right) \qquad (2.37)
$$
RNNでは

 - 時刻$t$の$\iota - 1$層目の隠れ状態ベクトル
 - 時刻$t-1$の$\iota$層目の隠れ状態ベクトル

を線形変換する。そのため、パラメータ行列$W^{(\iota)}$の要素数は$(N^{(\iota)} + N^{(\iota - 1)}) \times N^{(\iota)})$となる。なお、最初の隠れ状態ベクトル$h_{0}^{\iota}$は仮の隠れ状態ベクトルで、パラメータは零ベクトルなどの定数が使われることが多い。

時刻$t$での予測に使うスコア関数は、$h_{t}^{(L)}$を使って順伝播型ニューラルネットと同様に計算するのが基本である。
$$
f\left(x,y\right) = o_{o,y} \qquad (2.38)\\
o_{t} = W^{(o)} h_{t}^{(L)} + b^{(o)} \qquad (2.39)
$$

RNNは時間方向も階層と捉えると<font color="blue">時間方向に深いモデル</font>となっていることと、<font color="blue">時刻$t$によらず共通のパラメータを使っている</font>ことに特徴がある。

## 双方向再帰ニューラルネット
