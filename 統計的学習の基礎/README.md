# 『統計的学習の基礎』

## 2.3 予測のための二つの簡単なアプローチ
### 2.3.1 線形モデルと最小二乗法
入力ベクトル $X^{T} = \left( X_{1}, X_{2}, \cdots, X_{p} \right)$ が与えられたとき、出力 $Y$ を

$$
\hat {Y} = {\hat {\beta}}_{0} + \sum _{j=1}^{p}{ X_{j} {\hat {\beta}}_{j} }
$$

と予測する。

 - ${\hat {\beta}}_{0}$ : 切片( intercept ),機械学習の分野では**バイアス**( bias )と呼ばれることもある。

最小二乗法を用いた分類や回帰問題では

> 各クラスの訓練データはそれぞれ、平均が異なり相関のない2変数ガウス分布から生成されている

状況が適している。

### 2.3.2 最近傍法
最近傍法では、$\hat {Y}$ を予測する際に、入力$x$に最も近い訓練データ集合の観測値を利用する。すなわち、$k$最近傍法による $\hat {Y}$ の予測は
$$
\hat {Y}\left( x \right) = \frac {1}{k} \sum _{ x_{i} \in N_{k}\left(x\right) }{ y_{i} }
$$
と定義される。ここで、$N_{k}\left( x \right)$ は訓練データのうち $x$ に近い $k$ 個の点 $x_{i}$ によって定義される近傍を表している。

 - $k$最近傍法は近さの概念を用いるため、何らかの計量を定義する必要があるが、差し当たりユークリッド距離を考えておくことにする。

最小二乗法のパラメータ数が $p$ であるのに対し、$k$最近傍法のパラメータ数は近傍数$k$の一つであるように思える。これはこれで正しいのだが、最近傍法の**有効パラメータ数** ( effective number of parameters ) は $N/k$ であることが知られており、次元数$p$よりも一般的に大きく、$k$を増やすにつれて減少する。

### 2.3.3 最小二乗法から最近傍法へ
最小2乗法による線形決定境界は滑らかで、当てはめの結果も安定しているように見える。一方、決定境界が線形であるという仮定に過度に依存しているという問題点もあるように思われる。

現代のデータ分析に使われる方法の多くは、これまでに学んだ簡潔な二つの方法から派生したものと解釈できる。事実、最も簡単な方法とも言える1最近傍法は、低次元データ分析のエッセンスを多く含んでいる。以下に、これら二つの基本的な方法が、どのような観点から拡張されているかを整理する。

 - カーネル平滑化では重みの概念を導入する。$k$最近傍法は重みが0か1の離散値をとるものと解釈できるが、カーネル平滑化法では、中心点から離れるにつれて滑らかに減衰する重みが用いられる。

## 2.4 統計的決定理論
> $X \in R^{p}$ を実数の確率入力ベクトル、$Y \in R$ を実数の確率出力変数とし、これらが同時確率分布 $Pr\left( X, Y \right)$ に従うとする。入力$X$が与えられたとき、$Y$を予測する関数 $f\left( X \right)$ を見つけることが目標である。理論的に考察するためには、予測に対する罰則を定義するための**損失関数**(loss function) $L\left( Y, f\left(X\right) \right)$ を導入する必要がある。

>　損失関数のうち、**2乗誤差損失**(squared error loss) $L\left( Y,f\left(X\right) \right) = \left( Y - f\left(X\right) \right)^{2}$は有用で、最も頻繁に利用される。2乗誤差損失を用いると、$f$を選ぶ基準として、期待(2乗)予測誤差(expected (squared) prediction error)

$$
EPE\left( f \right) = \mathrm {E}\left( Y - f\left( X \right) \right)^{2}\\
= \int { \left[ f - f\left( x \right) \right]^{2} \mathrm {Pr}\left( dx,dy \right) }
$$

> を用いることになる。同時分布を $X$ で条件付けすることにより、EPEは

$$
EPE\left(f\right) = \mathrm {{E}_{X}} \mathrm {{E}_{Y|X}} \left( \left[ Y - f\left(X\right) \right]^{2} | X \right)
$$

> と表される。したがって、

<br>



> #### <font color="blue">加法的モデル</font> (additive model)
> 回帰モデルが
> $$
f\left( X \right) = \sum _{j=1}^{p}{f_{j}\left( X_{j} \right)}
$$
> の形式を持つと仮定する。

## 高次元での局所的手法

## 統計モデル、教師あり学習、関数近似
### 同時分布 $Pr\left( X,Y \right)$ のための統計モデル


### 教師あり学習


### 関数近似

> #### <font color="blue">線形基底展開</font> (linear basis expansion)
> $$
f_{\theta}\left(x\right) = \sum _{k=1}^{K}{ h_{k}\left(x\right) \theta_{k} }
$$


## 構造化回帰モデル


## 制限付き推定法
### 粗度に対する罰則とベイズ法

### カーネル法と局所回帰


### 基底関数と辞書による方法



## モデル選択と、バイアスと分散のトレードオフ
