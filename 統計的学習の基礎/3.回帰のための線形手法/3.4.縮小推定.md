# 縮小推定
変数選択法は、予測変数の一部だけを用いて残りをモデルから除外することで全変数を用いたモデルよりも予測誤差が(おそらく)低く、理解しやすいモデルを作成できる。しかし、これは離散的な処理(つまり変数を保持するか除外するかの二値)であるため、分散が大きくなることが多く、全変数モデルの予測誤差がなかなか削減できない。縮小推定はより連続的で、分散が大きいという問題に悩まされることがない。

## Ridge回帰
リッジ回帰では、回帰係数の大きさに罰則を課すことにより、係数の値を縮小させる。リッジ回帰係数は、罰則付き残差2乗和を最小化することで求められる。

$$
{\hat {\beta}}^{ridge} = \mathrm {arg} \min _{\beta}{\left\{ \sum _{i=1}^{N}{\left( y_{i} - \beta_{0} - \sum _{j=1}^{p}{x_{ij}\beta_{j}} \right)^{2}} + \lambda \sum _{j=1}^{p}{\beta_{j}^{2}} \right\}} \qquad (3.41)
$$

ここで、$\lambda \ge 0$ は縮小度合いを制御する複雑度パラメータであり、$\lambda$が大きくなると縮小度合いも大きくなる。また、係数は互いに0に向かって縮小される。

リッジ回帰問題は、次のように書くこともできる。

$$
{\hat {\beta}}^{ridge} = \mathrm {arg} \min _{\beta}{\sum _{i=1}^{N}{\left( y_{i} - \beta_{0} - \sum _{j=1}^{p}{x_{ij}\beta_{j}} \right)^{2}}} \qquad (3.42)\\
\mathrm {s.t.} \sum _{j=1}^{p}{ \beta_{j}^{2} } \le t
$$

この場合は、回帰係数に対する大きさの制約が明示的に示されている。式(3.41)の$\lambda$と式(3.42)の$t$には、一対一の対応がある。線形回帰モデルに相関のある説明変数が多く含まれている場合、係数の推定は不安定になり、分散が大きくなる(多重共線性問題)。ある変数に対する大きな正の係数は、その変数と相関の高い変数に対する大きな負の係数により打ち消されてしまうからである。式(3.42)のように係数の大きさに制約を課すことで、この問題は解決される。

