# 4 誤差逆伝播法

## 4.1 勾配計算の難しさ
確率的勾配降下法を実行するには、誤差関数$E\left( \mathrm {w} \right)$の勾配$\partial E\left( \mathrm {w} \right) / \partial \mathrm {w}$を計算する必要がある。

$$
{ \mathrm { w } }^{ (t+1) } = { \mathrm { w } }^{ (t) } - \epsilon { \nabla { E } }_{ i } \\
\nabla E = \frac { \partial E\left( \mathrm {w} \right) }{ \partial \mathrm {w} }
$$

 - 勾配のベクトルの各成分は、各層の$\partial E / \partial w_{ji}$と$\partial E / \partial b_{j}$

この誤差関数の微分計算は中間層、特に入力に近い深い層のパラメータほどその計算が面倒になる。

$$
\mathrm {u}^{(l + 1)} = \mathrm {W}^{(l + 1)} \mathrm {z}^{(l)} + \mathrm {b}^{(l + 1)}\\
\mathrm {z}^{(l + 1)} = f\left( \mathrm {u}^{(l + 1)} \right)
$$

ex)1つのデータ$x_{n}$に対する二乗誤差を第$l$層の重み$w^{(l)}_{ji}$で微分
$$
E_{n} = \frac {1}{2} \left[ f\left( x_{n} \right) - y_{n} \right]^{2} \\
\frac {\partial E_{n}}{ \partial {w}^{(l)}_{ji} } = \left[ f\left( x_{n} \right) - y_{n} \right] \frac {\partial f\left( x_{n} ; \mathrm {w} \right)}{\partial w^{(l)}_{ji}}
$$

順伝播型ニューラルネットのドキュメントで活性化関数は以下のように入れ子状態になっていることを示した。
$$
f\left( { W }^{ (L) }z^{ (L-1) }\left( x_{ n } \right) +{ b }^{ (L) } \right) \\ =f\left( { W }^{ (L) }f\left( { W }^{ (L-1) }z^{ (L-2) }\left( x_{ n } \right) +{ b }^{ (L-1) } \right) +{ b }^{ (L) } \right) \\ =f\left( { W }^{ (L) }f\left( { W }^{ (L-1) }f\left( \cdots f\left( { W }^{ (l) }z^{ (l-1) }\left( x_{ n } \right) +{ b }^{ (l) } \right) \cdots  \right) +{ b }^{ (L-1) } \right) +{ b }^{ (L) } \right) 
$$

このため、微分の連鎖規則を何度も繰り返す。

→プログラミングが大変面倒<br>
→計算量も大きくなる

この問題を解決する方法として、**<font color="blue">誤差逆伝播法</font>**(back propagation)がある。

※この問題は全学習データもしくはミニバッチに対する誤差関数の値に対しても言える。複数のデータに対する誤差関数の値は、各データに対する誤差関数の値の総和であるためである。

> 以降、表記を簡素化するために、下の図のように$+1$をいつも出力する特別な第0番ユニットを各層に導入し、バイアス$b_{j}$をそのユニットと各ユニット$j$との結合の重み$w^{l}_{0j} = b^{(l)}_{j}$と考えることとする。
>
> <img src="./imgs/04誤差逆伝播法/特別な第0番ユニット.png" width="60%">
>
> つまり、$l$層のユニットへの入力は、$l - 1$層の第0ユニットの出力が常に$z^{(l - 1)}_{0} = 1$となることで
$$
u^{l}_{j} = \sum _{i=1}^{M}{ w^{(l)}_{ji} z^{(l - 1)}_{i} } + b_{j} = \sum _{i=0}^{M}{ w^{(l)}_{ji} z^{(l - 1)}_{i} }
$$

>  - $M$ : 成分数
> 
> と簡潔に書ける。

---
## 4.2 ２層ネットワークでの計算


---
## 4.3 多層ネットワークへの一般化


---
## 4.4 勾配降下法の完全アルゴリズム
### 4.4.1 出力層でのデルタ


### 4.4.2 順伝播と逆伝播の行列計算


### 4.4.5 勾配の差分近似計算

---
## 4.5 勾配消失問題
