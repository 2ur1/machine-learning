# 3 確率的勾配降下法
## 3.1 勾配降下法

学習のゴール: 選んだ誤差関数(Error Function) $E\left(w\right)$ に対し、最小値を与える $w = arg\min _{ w }{ E\left( w \right)  } $を求めること

 - $E\left(w\right)$ は一般的な凸関数ではないので、絶対的(大域的)な極小点 $w$ を求めることは通常不可能
 - そのため, $E\left(w\right)$ の局所的な極小点 $w$ を求めることを考える

極小解は、何らかの初期値を出発点に $w$ を繰り返し更新する反復計算によって求める。そうする方法はいくつかあるが、中でも最も簡単な方法が**<font color="blue">勾配降下法</font>**(gradient descent method)。

勾配(gradient)が与えられたとき
$$
\nabla E\equiv \frac { \partial E }{ \partial w } =\left( \frac { \partial E }{ \partial { w }_{ 1 } } ,\cdots ,\frac { \partial E }{ \partial { w }_{ M } }  \right) ^{ T } \qquad (3.1)
$$

$$
w^{(t+1)} = w^{(t)} - \epsilon \nabla E \qquad (3.2)
$$

 - $\epsilon$ : **学習係数**(learning rate)

---
## 3.2 確率的勾配降下法

---
## 3.3 「ミニバッチ」の利用


---
## 3.4 汎化性能と過適合


---
## 3.5 過適合の緩和
### 3.5.1 正則化

### 3.5.2 重みの制約

### 3.5.3 ドロップアウト


---
## 3.6 学習のトリック

### 3.6.1 データの正規化


### 3.6.2 データ拡張


### 3.6.3 複数ネットの平均

### 3.6.4 学習係数の決め方

### 3.6.5 モメンタム


### 3.6.6 重みの初期化


### 3.6.7 サンプルの順序

