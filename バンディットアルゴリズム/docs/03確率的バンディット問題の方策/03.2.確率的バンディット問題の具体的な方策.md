# 具体的な方策
本節以降では、確率的バンディット問題において小さいリグレットを達成する方策の具体的な構成を行います.

前提事項
> 以下ではヘフディングの不等式あるいはチェルノフ・ヘフディングの不等式を適用するために、各アームからの報酬$X_{i}\left(t\right)$が$\left[0,1\right]$に含まれている場合を考えます. それ以外の確率分布モデルを考える場合も、そのモデルに応じて適切な指数関数型の確率不等式を使うことで、同様の方法で方策の構成を行うことができます.

<br>

> 以下では、アーム$i$からの報酬の標本平均を$\hat {\mu}_{i}$で表し、特に時刻$t$の開始時点での標本平均であることを明確にする場合は$\hat {\mu}_{i}\left( t \right)$、すなわち
$$
\hat {\mu}_{i}\left(t\right) = \cfrac {1}{N_{i}\left(t\right)} \sum _{s\in \left\{ 1,2,\dots ,t-1 \right\} :i\left( s \right) =i}{X_{i}\left(s\right)}
$$
> と表記します. また、アーム$i$を$n$回引いた時点でのアーム$i$の標本平均を$\hat {\mu}_{i,n}$と表し、このとき$\hat {\mu}_{i}\left(t\right) = \hat {\mu}_{i,Ñ_{i}\left(t\right)}$が成り立ちます. 標本平均が最大のアームを$\hat {i}^{\ast} = \hat {i}^{\ast}\left(t\right) = arg \max _{i}{\hat {\mu}_{i}\left(t\right)}$と表します.

## ε-貪欲法(Epsilon-Greedy method)

<table>
	<tr>
		<th>項目</th>
		<th>説明</th>
	</tr>
	<tr>
		<td><b>概要</b></td>
		<td>ε-貪欲法では、まず全体のアーム選択回数$T$のうち$\epsilon T$回を探索期間としてすべてのアームを一様に選択し、残りの$(1 - \epsilon)T$回を活用期間としてそれまでで最も標本平均の高かったアーム$\hat {i^{\ast}}$を引き続けるものです.</td>
	</tr>
	<tr>
		<td><b>メリット</b></td>
		<td>実装が容易</td>
	</tr>
	<tr>
		<td><b>デメリット</b></td>
		<td>UCB方策に比べて、性能は悪い場合が多い</td>
	</tr>
</table>

### アルゴリズム
パラメータ : $\epsilon > 0$.<br>
入力 : 全体のアーム選択回数$T$.<br>

 1. [探索(exploration)] すべてのアーム$i$を$\epsilon T / K$回引く.
 2. [知識利用(exploitation)] アーム$\hat {i^{\ast}}\left( \epsilon T + 1 \right) = arg \max _{i}{\hat {\mu}_{i}^{\ast}\left( \epsilon T + 1 \right)}$を$(1 - \epsilon)T$回引く.

なお、上記のアルゴリズムでは時刻$\epsilon T + 1$以降に引くアームを$\hat {i^{\ast}}\left( \epsilon T + 1 \right)$に固定している. これを各時刻での期待値最大のアーム$\hat {i^{\ast}}\left(t\right) = arg \max _{i}{\hat {\mu_{i}}\left( t \right)}$を引くように変更することで性能を若干改善できるが、以下で議論する本質的なε-貪欲法の性質については変わらない.

<table>
	<tr>
		<th>指標</th>
		<th>評価</th>
	</tr>
	<tr>
		<td>Curiosity(好奇心)</td>
		<td>常にすべてのアームが選ばれる確率が$\epsilon T / K$で等しい</td>
	</tr>
	<tr>
		<td>時間経過による知識利用<br>(Increased Exploitation over Time)</td>
		<td>$\epsilon$の値は一定なので、知識利用(exploitation)は増加しない</td>
	</tr>
	<tr>
		<td>Strategic Exploration</td>
		<td>-</td>
	</tr>
	<tr>
		<td>ハイパーパラメータ数</td>
		<td>$\epsilon$</td>
	</tr>
	<tr>
		<td>初期戦略(Initialization Strategy)</td>
		<td>すべてのアームが同じ評価</td>
	</tr>
	<tr>
		<td>Context-Aware</td>
		<td>コンテキストは一切考慮しない</td>
	</tr>
</table>

> ### [定理3.3] ε-貪欲法のリグレット上界
> 任意の$\epsilon \in \left(0,1\right)$について
$$
E\left[ \mathrm {regret}\left(T\right) \right] \le \sum _{i \neq i^{\ast}}{ \Delta_{i}\left( \cfrac {\epsilon T}{K} + \mathrm {exp}\left( \log {T} - \cfrac {\epsilon \Delta_{i}^{2} T }{2K} \right) \right) }
$$
> が成り立つ. 特に、$c \le \min _{i \neq i^{\ast}}{ \Delta_{i}^{2} }$に対して$\epsilon = \frac {2K\log {T}}{cT}$とすると
$$
E\left[ \mathrm {regret}\left(T\right) \right] \le \sum _{i \neq i^{\ast}}{ \Delta_{i} \left( \cfrac {2\log {T}}{c} + 1 \right) }
$$
> が成り立つ.

**証明**<br>
WIP

### ε-貪欲法のまとめ
この定理から、ε-貪欲法においては$\epsilon$を適切に調整することで理論限界と同じく$O\left( \log {T} \right)$のリグレットを達成できることがわかります.
WIP

---
## UCB方策(Upper Confidence Bound)

<table>
	<tr>
		<th>項目</th>
		<th>説明</th>
	</tr>
	<tr>
		<td><b>概要</b></td>
		<td></td>
	</tr>
	<tr>
		<td><b>メリット</b></td>
		<td>①どの程度アームについて知っているかを考慮してアームを選択できる ②パラメータを設定する必要がない ③最終的に最も良いアームのみを選ぶように収束する</td>
	</tr>
	<tr>
		<td><b>デメリット</b></td>
		<td>悪いアームを探索のために引きすぎてしまう</td>
	</tr>
</table>

**UCBスコア**<br>
この方策では、UCBスコア$\bar {\mu}_{i}\left(t\right)$を各時刻ごとに計算し、そのスコアが最も高いアームを引きます.<br>
UCBスコアの取り方には様々な種類がありますが、ここではヘフディングの不等式に基づいたものを用います.
$$
\bar {\mu}_{i}\left( t \right) = \hat {\mu}_{i}\left( t \right) + \sqrt {\frac {\log {t}}{ 2 N_{i}\left(t\right) }}
$$

このUCBスコアは標本平均$\hat {\mu}_{i}\left( t \right)$に補正項を上乗せした値になっています. 選択数$N_{i}\left( t \right)$が少ないアームほどこの補正項は大きくなるので、標本平均が小さいとしても、その標本数が少ないアームは引かれやすくなります.

### アルゴリズム

 1. すべてのアームを1回ずつ引く
 2. **`for`** $t = K+1 ,K+2, \dots, T$ **`do`**
 3. $\qquad $各アーム$i$のUCBスコア$\bar {\mu}_{i}\left( t \right) = \hat {\mu}_{i}\left( t \right) + \sqrt {\frac {\log {t}}{ 2 N_{i}\left(t\right) }}$を計算.
 4. $\qquad $スコアが最大のアーム$arg \max _{i \in \left\{ 1,2,\dots,K \right\}}{ \bar {\mu}_{i}\left( t \right) }$を引く. (スコア最大のアームが複数ある場合にそのいずれかを選ぶかは任意.)
 5. **`end for`**


### 補正項の導出
WIP

> ### UCB方策のリグレット上界
> UCB方策のもとで、任意の$\epsilon \in \left( 0, \Delta_{i} \right)$について
$$
E\left[ \mathrm {regret}\left( T \right) \right] \le \sum _{i \neq i^{\ast}}{ \Delta_{i} \left( \frac { \log { T }  }{ 2{ \left( { \Delta  }_{ i }-2\epsilon  \right)  }^{ 2 } } +\frac { 3 }{ 2{ \epsilon  }^{ 2 } } +\frac { \log { \cfrac { 1 }{ 2{ \epsilon  }^{ 2 } }  }  }{ 4{ \epsilon  }^{ 2 } } \right) }
$$
> が成り立つ. 特に、$\epsilon = O\left( \left( \log {T} \right)^{-1/3} \right)$とすることにより
$$
E\left[ \mathrm {regret}\left( T \right) \right] \le \sum _{i \neq i^{\ast}}{ \frac {\log {T}}{2\Delta_{i}} + O\left( { \left( \log { T }  \right)  }^{ 2/3 }\log { \log { T }  }  \right) }
$$
> が成り立つ.

**証明**<br>
WIP

---
## KL-UCB方策
UCB方策アルゴリズムで用いたUCBスコアはヘフディングの不等式に基づくものでしたが、**より精密な確率の上界を与えるチェルノフ・ヘフディングの不等式**を用いても同様にUCBスコアを導出できる.

**KL-UCBスコア**<br>
上記のUCBスコアはKLダイバージェンスを用いて
$$
\begin{eqnarray}
\bar {\mu}_{i}^{\prime}\left( t \right) & = & \max {\left\{ {\mu} : {e^{-2n_{i}d\left( \hat {\mu}_{i}\left(t\right), \mu \right)} \le 1/t} \right\}}\\
& = & \max {\left\{ {\mu} : {-2n_{i}d\left( \hat {\mu}_{i}\left(t\right), \mu \right)} \ge \log {t} \right\}}
\end{eqnarray}
$$
と表される.

### アルゴリズム
UCB方策アルゴリズムのUCBスコアをKL-UCBスコアで置き換えたもの.




---
## MED方策



---
## ソフトマックス方策

<table>
	<tr>
		<th>項目</th>
		<th>説明</th>
	</tr>
	<tr>
		<td><b>概要</b></td>
		<td>当たる確率の高いアームを高い確率、当たる確率の低いアームを低い確率で引くことによって、活用と探索を行う.</td>
	</tr>
	<tr>
		<td><b>メリット</b></td>
		<td>良いアームと悪いアームがあった場合に、その情報が探索に使用される => 良いアームを多く引き、悪いアームを少なく引くことができる.</td>
	</tr>
	<tr>
		<td><b>デメリット</b></td>
		<td>①引いた回数を考慮しない②パラメータの設定によって、結果が大幅に異なる③探索がどれだけ進んでも一定の確率で悪いアームを引いてしまう</td>
	</tr>
</table>


---
## トンプソン抽出


---
# 3.6 最悪時の評価




---
# バンディットアルゴリズムの改良方法

## 1. 焼きなまし(Annealing)方法
> 最初は探索の割合を多くして、徐々に活用の割合を増やしていく.

### ex)ε-貪欲法
最初は$\epsilon$(活用の割合)を低く設定し、徐々に1に近づけていく

### ex)ソフトマックス方策
最初は$\tau$($\tau = \infty$でランダム)の値を高く設定し、徐々に$0$に近づけていく.

## 2. 初期値の設定
> 1回もアームを引いていないスロットマシンに、適切な初期値を与える. 事前情報がある場合に上手く利用したい.






